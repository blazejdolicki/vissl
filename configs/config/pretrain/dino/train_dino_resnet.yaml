# @package _global_
# Config adapted from train_dino.yaml with resnet instead of vision transformer and based on
# https://dl.fbaipublicfiles.com/dino/dino_resnet50_pretrain/args.txt
config:
  VERBOSE: False
  LOG_FREQUENCY: 10
  TEST_ONLY: False
  TEST_MODEL: False
  SEED_VALUE: 0
  MULTI_PROCESSING_METHOD: forkserver
  HOOKS:
    PERF_STATS:
      MONITOR_PERF_STATS: True
      PERF_STAT_FREQUENCY: 40
      ROLLING_BTIME_FREQ: 5
    TENSORBOARD_SETUP:
      EXPERIMENT_LOG_DIR: tensorboard
      FLUSH_EVERY_N_MIN: 5
      LOG_DIR: .
      LOG_PARAMS: true
      LOG_PARAMS_EVERY_N_ITERS: 310
      LOG_PARAMS_GRADIENTS: true
      USE_TENSORBOARD: true
  DATA:
    NUM_DATALOADER_WORKERS: 5 # because --cpus-per-task=5 in train_nct_dino.job
    TRAIN: # TODO change the dataset
      DATA_SOURCES: [disk_folder]
      DATASET_NAMES: [imagenet1k_folder]
      BATCHSIZE_PER_REPLICA: 64 # 64 is fine for titan RTX but for GeForce 1080Ti decrease it to avoid CUDA OOM
      LABEL_TYPE: sample_index    # just an implementation detail. Label isn't used
      TRANSFORMS:
        - name: ImgPilToMultiCrop
          total_num_crops: 8
          size_crops: [224, 96]
          num_crops: [2, 6]
          crop_scales: [[0.14, 1], [0.05, 0.14]]
        - name: RandomHorizontalFlip
          p: 0.5
        - name: ImgPilColorDistortion
          strength: 0.5
        - name: ImgPilMultiCropRandomApply
          transforms: [{"name": "ImgPilGaussianBlur", "p": 1., "radius_min": 0.1, "radius_max": 2.0}]
          prob: [1., 0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]
        - name: ImgPilMultiCropRandomApply
          transforms: [{"name": "ImgPilRandomSolarize", "p": 1.}]
          prob: [0., 0.2, 0., 0., 0, 0, 0, 0]
        - name: ToTensor
        - name: Normalize # TODO change means and stds from ImageNet to our dataset
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]
      COLLATE_FUNCTION: multicrop_collator
      MMAP_MODE: True
      COPY_TO_LOCAL_DISK: False
      COPY_DESTINATION_DIR: /tmp/imagenet1k/
      DROP_LAST: True
  TRAINER:
    TRAIN_STEP_NAME: standard_train_step
  METERS:
    name: ""
  MODEL:
    TRUNK:
      NAME: resnet
      RESNETS:
        DEPTH: 50
    HEAD:
      PARAMS: [
              # dims based on the head of ResNet50 when running `python main_dino.py --arch resnet50`
              ["swav_head", {"use_weight_norm_prototypes": True, "dims": [2048, 2048, 2048, 256], "use_bn": True, "return_embeddings": False, "activation_name": "GELU", "num_clusters": [60000]}],
      ]
    # the model parameters that should be frozen for certain specific number of iterations.
    # i.e the parameters are frozen for specified iterations and then start training.
    # DINO in the args of RN50 in the repo freeze the last (output) layer for the first epoch.
    # Typically doing so during the first epoch helps training. Try freezing it for more epochs if the loss does not decrease.
    TEMP_FROZEN_PARAMS_ITER_MAP: [
      ['module.heads.0.prototypes0.weight_v', 1251],
      ['module.heads.0.prototypes0.weight_g', 1251],
      ]
    AMP_PARAMS:
      AMP_TYPE: pytorch
      USE_AMP: False
  LOSS:
    name: dino_loss
    dino_loss:
      momentum: 0.9995 # higher value due to smaller batch
      teacher_temp_warmup_iters: 20000 # try 20 epochs - Paper uses 50 epochs for RN50
      teacher_temp_min: 0.02
      teacher_temp_max: 0.07
      ema_center: 0.9
      normalize_last_layer: true
  OPTIMIZER:
      name: adamw
      momentum: 0.9
      nesterov: False
      num_epochs: 2 # TODO increase number of epochs later
      regularize_bn: False
      regularize_bias: False
      param_schedulers:
        lr_head:
          name: composite
          schedulers:
            - name: linear
              start_value: 0.0 # based on the implementation of cosine_scheduler() in DINO repo
              end_value: 0.3 # determined according to formula 0.3*(batch_size/256)
            - name: cosine
              start_value: 0.3 # determined according to formula 0.3*(batch_size/256)
              end_value: 0.28
          update_interval: epoch
          interval_scaling: [rescaled, fixed]
          lengths: [0.4, 0.6]
        lr:
          name: composite
          schedulers:
            - name: linear
              start_value: 0.0 # based on the implementation of cosine_scheduler() in DINO repo
              end_value: 0.3 # determined according to formula 0.0005*(batch_size/256)
            - name: cosine
              start_value: 0.3 # determined according to formula 0.0005*(batch_size/256)
              end_value: 0.28
          update_interval: epoch
          interval_scaling: [rescaled, fixed]
          lengths: [0.4, 0.6]
        weight_decay:
          name: cosine
          start_value: 0.000001
          end_value: 0.000001
          update_interval: epoch
        weight_decay_head:
          name: cosine
          start_value: 0.000001
          end_value: 0.000001
          update_interval: epoch
  DISTRIBUTED: # TODO this might need to be changed
    BACKEND: nccl
    NUM_NODES: 1
    NUM_PROC_PER_NODE: 1
    INIT_METHOD: tcp
    RUN_ID: auto
    NCCL_DEBUG: true
  MACHINE:
    DEVICE: gpu
  CHECKPOINT:
    DIR: "."
    AUTO_RESUME: True
    CHECKPOINT_FREQUENCY: 1
    OVERWRITE_EXISTING: true

